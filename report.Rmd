---
title: "Final Project"
output:
  pdf_document:
    latex_engine: xelatex
    extra_dependencies: ["float"]
header-includes:
  - \usepackage{setspace}
  - \usepackage{fontspec}
  - \setmainfont{Times New Roman}
fontsize: 12pt
geometry: margin=1in
---
\setstretch{2}  

# Abstract

a brief introduction, brief description of methods, and main results into a one-paragraph summary

# Introduction

This project is based on a dataset which includes three test scores (math, reading and writing) of students at a public school and a variety of personal and socio-economic factors that may have interaction effects upon them. We want to use these factors as the covariates to predict Math, Reading and Writing scores.

# Methods (data description and statistical methods)

## Data Description and Visualization
There are a total of 14 variables(including 3 response variables[12-14] and 11 predictor variables[1-11]) with 948 observations in this dataset. The variables and description is listed below:

1. Gender: Gender of the student (male/female)

2. EthnicGroup: Ethnic group of the student (group A to E)

3. ParentEduc: Parent(s) education background (from some_highschool to master's degree)

4. LunchType: School lunch type (standard or free/reduced)

5. TestPrep: Test preparation course followed (completed or none)

6. ParentMaritalStatus: Parent(s) marital status (married/single/widowed/divorced)

7. PracticeSport: How often the student parctice sport (never/sometimes/regularly))

8. IsFirstChild: If the child is first child in the family or not (yes/no)

9. NrSiblings: Number of siblings the student has (0 to 7)

10. TransportMeans: Means of transport to school (schoolbus/private)

11. WklyStudyHours: Weekly self-study hours(less that 5hrs; between 5 and 10hrs; more than 10hrs)

12. MathScore: math test score(0-100)

13. ReadingScore: reading test score(0-100)

14. WritingScore: writing test score(0-100)

The 11 predictor variables[1-11] are categorical variables and the 3 response variables[12-14] are continuous variables. 

The 3 response variables all have a relatively symmetric distributions with several outliers.(See figure 1)

## Marginal Distributions and Pairwise Relationship 
From the pariplot of marginal distributions, we can see that there is no obvious nonlinearities.(See figure 3)

## Correlation between varibales
We use the following methods to assess the strength of correlation:

For categorical vs categorical variables, we use Cramer's V correlation.

For continuous vs continuous variables, we use Pearson correlation.

For categorical vs continuous variables, we use ANOVA(mcor).

This correlation coefficient all varies from 0 (corresponding to no association between the variables) to 1 (complete association) and can reach 1 only when each variable is completely determined by the other.

From the table and the heatmap we generated, we can see that there is no colinearities exist between response variables and explanatory variables. There are also colinearities exist within 11 explanatory variables. However, there is a high correlation between the 3 response variables.Each explanatory are correlated with the other two with a correlation coefficient higher than 0.8.

## Missing Value Treatment:

Minimal missing values were observed as figure 1, primarily in qualitative variables such as EthnicGroup, ParentEduc, TestPrep, and others. Mode imputation was used for all except TransportMeans, with samples still showing missing values after imputation excluded. 

## Model Selection:

Interaction terms were included in the full models, despite limited variable correlations, for theoretical and practical reasons, covering all 11 predictors and their interactions to reduce overfitting. Model refinement began with AIC-guided backward elimination, but to address the retention of excessive variables, we applied LASSO penalization with cross-validation for optimal lambda selection. This process eliminated interaction terms with shrinkage coefficients below 0.5, resulting in three more streamlined, nested models (detailed in the appendix).

## Model Assessment:

We utilized various diagnostic plots, including residual vs. fitted, Q-Q plots, scale-location, and residuals vs. leverage, to check whether our model meets the assumptions. We also computed Cook's Distance to identify any influential observations. Furthermore, to assess the presence of multicollinearity issues, we calculated the model's adjusted Generalized Variance Inflation Factor (GVIF).

## Model Validation:

We employed a 10-fold cross-validation approach, which involved systematically partitioning the data into 10 subsets. In each cycle of this method, 9 subsets were used for training the model while the remaining one served as the validation set. This rotation continued until each subset had been utilized for validation, ensuring a comprehensive assessment of the model's performance. Additionally, to evaluate the predictive accuracy of our models, we calculated the Mean Squared Prediction Error (MSPE) using separate test data.

# Results

There are a total of 14 variables(including 3 response, continuouse variables[12-14] and 11 predictor, categorical variables[1-11]) with 948 observations in this dataset. After missing value treatment, we left 846 obserbvations for further analysis. The 3 response variables all have a relatively symmetric distributions with several outliers.

There is no colinearities exist between response variables and explanatory variables, and within 11 explanatory variables. However, there is a high correlation between the 3 response variables.Each explanatory are correlated with the other two with a correlation coefficient higher than 0.8.

To facilitate internal validity assessment in subsequent modeling, the data was split into a training set (80%) and a test set (20%).

Diagnostic plots for these models indicate no issues. Specifically, examination of the residual vs leverage plots for the three models revealed a few outlier observations, notably in samples 181 and 268 according to figure 2. However, closer inspection showed that their leverage did not exceed 0.5, and Cook's distances were below 0.1. Additionally, no data entry anomalies were identified in these samples. Removing these samples and reconstructing the models showed negligible differences from the original models. Therefore, no adjustments were made, and the original data was used for modeling. As for the multicollinearity check, we found no substantial multicollinearity concerns in any of the model variables.

The cross-validation results indicated that the RMSE for the Math model was concentrated around 12, while for the Reading and Writing models, RMSE values were centered around 12.5, according to figure 3. To test for potential overfitting, the initially separated test data was employed to evaluate the predictive performance of the models. The Mean Squared Prediction Errors (MSPE) for the Math, Reading, and Writing models were 198.3466, 152.9267, and 142.8281, respectively. Contrary to the RMSE results, the Math model exhibited a lower performance compared to the Reading and Writing models. Given that the Math model incorporated the most predictors, it suggests a potential issue with overfitting.

# Conclusions/Discussion

Your content here. 

# A brief summary on each group member’s contribution

Aiying Huang (ah4167) notably contributed to the project by handling literature research for methods, model construction and validation, and structuring the final report. Mia Yu (my2838) played a key role in model selection, additional methods research, and enhancing model diagnostics. Eunice Wang (cw3555) was crucial in initial data analysis, variable selection, relationship exploration, and writing the report's introduction and conclusion.

\newpage

# Figures and Tables

```{r, echo=FALSE, fig.cap="Ys' distribution", out.width='6in', fig.pos="H"}
knitr::include_graphics("Y'distribution.png")
```

```{r, echo=FALSE, fig.cap="Ys' correlation", out.width='6in', fig.pos="H"}
knitr::include_graphics("Y's correlation.png")
```

```{r, echo=FALSE, fig.cap="pairwise", out.width='6in', fig.pos="H"}
knitr::include_graphics("pairwise.png")
```

```{r, echo=FALSE, fig.cap="heatmap", out.width='6in', fig.pos="H"}
knitr::include_graphics("categoricalcorr.png")
```

```{r, echo=FALSE, fig.cap="missingdata", fig.pos="H"}
knitr::include_graphics("Missingdata.png")
```

```{r, echo=FALSE, fig.cap="math", out.width='7in', fig.pos="H"}
knitr::include_graphics("math.png")
```

```{r, echo=FALSE, fig.cap="reading", out.width='7in', fig.pos="H"}
knitr::include_graphics("reading.png")
```

```{r, echo=FALSE, fig.cap="writing", out.width='7in', fig.pos="H"}
knitr::include_graphics("writing.png")
```

```{r, echo=FALSE, fig.cap="Cook's Distance", out.width='8in', fig.pos="H"}
knitr::include_graphics("cookdistance.png")
```

```{r, echo=FALSE, fig.cap="CV outcome", out.width='8in', fig.pos="H"}
knitr::include_graphics("cv.png")
```

```{r table-mspe_values, echo=FALSE}
mspe_values <- data.frame(
  Subject = c("Math", "Reading", "Writing"),
  MSPE = c(198.3466, 152.9267, 142.828)
)
knitr::kable(mspe_values, col.names = c("Subject", "MSPE"), caption = "MSPE Values for Different Subjects")
```

```{r table-math, echo=FALSE}
#| echo: false
#| message: false
library(dplyr)
# Your provided data frames
vif_values_math <- data.frame(
  Predictor = c("Gender", "EthnicGroup", "ParentEduc", "LunchType", "TestPrep", "ParentMaritalStatus",
                "PracticeSport", "TransportMeans", "WklyStudyHours"),
  GVIF = c(1.655040, 1.353349, 1.081339, 1.643025, 1.074470, 1.331176, 1.357566, 1.516250, 1.366449)
)


knitr::kable(vif_values_math, caption = "Variance Inflation Factors for Math Model", 
      col.names = c("Predictor", "GVIF"), 
      format = "latex", booktabs = TRUE)

```

```{r table-writing, echo=FALSE}
vif_values_writing <- data.frame(
  Predictor = c("Gender", "EthnicGroup", "ParentEduc", "LunchType", "TestPrep", "ParentMaritalStatus",
                "PracticeSport", "IsFirstChild", "NrSiblings", "TransportMeans", "WklyStudyHours"),
  GVIF = c(1.042331, 1.041528, 1.157013, 1.659319, 1.040662, 1.183376, 1.200553, 1.339793, 1.040662, 
           1.034014, 1.413038)
)
# Writing Model VIF Values
knitr::kable(vif_values_writing, caption = "Variance Inflation Factors for Writing Model", 
      col.names = c("Predictor", "GVIF"), 
      format = "latex", booktabs = TRUE)
```

```{r table-reading, echo=FALSE}
vif_values_reading <- data.frame(
  Predictor = c("Gender", "EthnicGroup", "ParentEduc", "LunchType", "TestPrep", "ParentMaritalStatus",
                "PracticeSport", "IsFirstChild", "NrSiblings", "TransportMeans", "WklyStudyHours"),
  GVIF = c(1.036102, 1.039638, 1.032325, 1.648075, 1.044683, 1.117825, 1.199588, 1.619041, 1.608364, 
           1.034064, 1.255155)
)
# Reading Model VIF Values
knitr::kable(vif_values_reading, caption = "Variance Inflation Factors for Reading Model", 
      col.names = c("Predictor", "GVIF"), 
      format = "latex", booktabs = TRUE)
```


\newpage

# References

Bollinger, G. (1981). Book Review: Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Journal of Marketing Research, 18(3), 392-393. https://doi.org/10.1177/002224378101800318

Fox, J., & Monette, G. (1992). Generalized Collinearity Diagnostics. Journal of the American Statistical Association, 87(417), 178-183. https://doi.org/10.2307/2290467

Tibshirani, R. (1996). Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267-288.

Hyndman, R. J., & Athanasopoulos, G. (2018). Forecasting: Principles and Practice (2nd ed.).OTexts.

\newpage

# Appendix

## Three final regression models-看要不要加有coefficient的版本

`MathScore ~  Gender + EthnicGroup + ParentEduc + LunchType + TestPrep + ParentMaritalStatus + PracticeSport + TransportMeans + WklyStudyHours + Gender:PracticeSport + EthnicGroup:ParentEduc + ParentEduc:ParentMaritalStatus + + ParentEduc:PracticeSport + LunchType:PracticeSport + ParentMaritalStatus:TransportMeans + PracticeSport:WklyStudyHours`

`ReadingScore ~ Gender + EthnicGroup + ParentEduc + LunchType + TestPrep + ParentMaritalStatus + PracticeSport + IsFirstChild + NrSiblings + TransportMeans + WklyStudyHours + LunchType:PracticeSport + ParentMaritalStatus:PracticeSport + ParentMaritalStatus:IsFirstChild + PracticeSport:WklyStudyHours + NrSiblings:WklyStudyHours`

`WritingScore ~ Gender + EthnicGroup + ParentEduc + LunchType + TestPrep + ParentMaritalStatus + PracticeSport + IsFirstChild + NrSiblings + TransportMeans + WklyStudyHours + ParentEduc:IsFirstChild + LunchType:PracticeSport + TestPrep:NrSiblings + ParentMaritalStatus:PracticeSport + ParentMaritalStatus:IsFirstChild + PracticeSport:WklyStudyHours + IsFirstChild:WklyStudyHours`

